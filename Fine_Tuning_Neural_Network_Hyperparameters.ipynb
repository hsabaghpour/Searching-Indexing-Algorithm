{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOwRjb0j1v2QI/b5Vb/Qq1k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hsabaghpour/Searching-Indexing-Algorithm/blob/main/Fine_Tuning_Neural_Network_Hyperparameters.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-lFh3NrhZZXR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VObWGabwZYSl",
        "outputId": "ffb608f4-7f12-4085-8054-dc82232b5b33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.5/129.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m950.8/950.8 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -q -U keras-tuner"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can use the keras_tuner library, often imported as kt, to create a function that builds, compiles, and returns a Keras model. This function takes a kt.HyperParameters object as an argument, which defines hyperparameters like the number of hidden layers, neurons per layer, learning rate, and optimizer type. These hyperparameters are used to configure the model. For example, you can use it to create an MLP for classifying Fashion MNIST images with various hyperparameter options.\n",
        "\n",
        "In summary, you can use kt to build flexible Keras models with adjustable hyperparameters for tasks like image classification.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5_BObdq3bxVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import keras_tuner as kt\n",
        "\n",
        "\n",
        "\n",
        "def build_model(hp):\n",
        "    n_hidden = hp.Int(\"n_hidden\", min_value=0, max_value=8, default=2)\n",
        "    n_neurons = hp.Int(\"n_neurons\", min_value=16, max_value=256)\n",
        "    learning_rate = hp.Float(\"learning_rate\", min_value=1e-4, max_value=1e-2,\n",
        "                             sampling=\"log\")\n",
        "    optimizer = hp.Choice(\"optimizer\", values=[\"sgd\", \"adam\"])\n",
        "    if optimizer == \"sgd\":\n",
        "        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
        "    else:\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    for _ in range(n_hidden):\n",
        "        model.add(tf.keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
        "    model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n",
        "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
        "                  metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xoJ2Fn0JZur2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you're interested in performing a simple random search, you can set up a random search tuner with kt.RandomSearch. You provide the build_model function to the tuner's constructor and then invoke the search() method.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XjRsMvQEcDXg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function defines hyperparameters for a machine learning model. For instance, it checks if \"n_hidden\" is already defined; if not, it creates it as an integer between 0 and 8 (defaulting to 2 if not set). \"n_neurons\" is handled similarly. \"learning_rate\" is registered as a floating-point number between 10^-4 and 10^-2. The optimizer is either \"sgd\" or \"adam\" (defaulting to \"sgd\"). Depending on the optimizer, an SGD or Adam optimizer with the specified learning rate is created.\n",
        "\n",
        "In summary, this function manages hyperparameters for a model, providing default values and allowable ranges for each hyperparameter. It's a way to configure and experiment with different model settings.\n",
        "\n",
        "\n",
        "The second part of the function constructs the model based on the hyperparameter values. It starts with a Sequential model, adds a Flatten layer, followed by the specified number of hidden layers with ReLU activation, and an output layer with 10 neurons (one for each class) using the softmax activation. The model is then compiled and returned.\n",
        "\n"
      ],
      "metadata": {
        "id": "xvvZmfAHcy0k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()\n",
        "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist\n",
        "X_train, y_train = X_train_full[:-5000], y_train_full[:-5000]\n",
        "X_valid, y_valid = X_train_full[-5000:], y_train_full[-5000:]\n",
        "\n",
        "\n",
        "\n",
        "random_search_tuner = kt.RandomSearch(\n",
        "    build_model, objective=\"val_accuracy\", max_trials=5, overwrite=True,\n",
        "    directory=\"my_fashion_mnist\", project_name=\"my_rnd_search\", seed=42)\n",
        "random_search_tuner.search(X_train, y_train, epochs=10,\n",
        "                           validation_data=(X_valid, y_valid))\n"
      ],
      "metadata": {
        "id": "n1xRo4S9Z_Vg",
        "outputId": "2c3e5289-1e2c-416b-ee4a-b52916292d6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 5 Complete [00h 01m 12s]\n",
            "val_accuracy: 0.8353999853134155\n",
            "\n",
            "Best val_accuracy So Far: 0.8628000020980835\n",
            "Total elapsed time: 00h 08m 02s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "running 5 more trials: this means you don’t have to run all the trials in one shot. Lastly, since objective is set to \"val_accuracy\", the tuner prefers models with a higher validation accuracy, so once the tuner has finished searching, you can get the best models like this:"
      ],
      "metadata": {
        "id": "sM8F-B5gdm9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top3_models = random_search_tuner.get_best_models(num_models=3)\n",
        "best_model = top3_models[0]\n"
      ],
      "metadata": {
        "id": "b4c8O5bxcSjg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#You can also call get_best_hyperparameters() to get the kt.HyperParameters of the best models:\n",
        "\n",
        "\n",
        "top3_params = random_search_tuner.get_best_hyperparameters(num_trials=3)\n",
        "top3_params[0].values  # best hyperparameter values\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQMAdnevdzg9",
        "outputId": "69207b73-4341-4820-b627-515abe069538"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'n_hidden': 7,\n",
              " 'n_neurons': 100,\n",
              " 'learning_rate': 0.0012482904754698163,\n",
              " 'optimizer': 'sgd'}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_trial = random_search_tuner.oracle.get_best_trials(num_trials=1)[0]\n",
        "best_trial.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKi-1jxOerzr",
        "outputId": "1a74acd9-ca36-453f-bfbf-e5c1dea2e31d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 1 summary\n",
            "Hyperparameters:\n",
            "n_hidden: 7\n",
            "n_neurons: 100\n",
            "learning_rate: 0.0012482904754698163\n",
            "optimizer: sgd\n",
            "Score: 0.8628000020980835\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#You can also access all the metrics directly:\n",
        "\n",
        "\n",
        "\n",
        "best_trial.metrics.get_last_value(\"val_accuracy\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJS8je6AeUXs",
        "outputId": "fd03ecdb-05cb-43e7-dcc7-ec411a80d630"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8628000020980835"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#If you are happy with the best model’s performance, you may continue training it for a few epochs on the full training set (X_train_full and y_train_full), then evaluate it on the test set, and deploy it to production\n",
        "\n",
        "\n",
        "best_model.fit(X_train_full, y_train_full, epochs=10)\n",
        "test_loss, test_accuracy = best_model.evaluate(X_test, y_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZyhBwMYewwV",
        "outputId": "c324da26-ed8f-40ba-c03c-2596d3a00017"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.3253 - accuracy: 0.8803\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.3158 - accuracy: 0.8831\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3082 - accuracy: 0.8843\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.3018 - accuracy: 0.8887\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2945 - accuracy: 0.8899\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2890 - accuracy: 0.8920\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2839 - accuracy: 0.8945\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2781 - accuracy: 0.8963\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2739 - accuracy: 0.8982\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2686 - accuracy: 0.9004\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.3991 - accuracy: 0.8597\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In some situations, you might need to fine-tune data preprocessing or model training settings, like the batch size. To do this, you use a different approach: you create a subclass of the kt.HyperModel class and define two methods - build() and fit().\n",
        "\n",
        "The build() method is similar to the build_model() function and specifies the model architecture using hyperparameters.\n",
        "The fit() method takes hyperparameters, a compiled model, training data, and model.fit() arguments. It trains the model and returns the training history. This method can use hyperparameters to make decisions about data preprocessing, batch size, and more.\n",
        "For instance, the provided code defines a class that builds the same model as before with similar hyperparameters. It introduces a Boolean \"normalize\" hyperparameter, which controls whether the training data should be standardized before fitting the model.\n",
        "\n",
        "In summary, when subclassing the kt.HyperModel class, you can customize how your model is built and trained, including data preprocessing, based on hyperparameter settings.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZZ5a1Epgf918"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyClassificationHyperModel(kt.HyperModel):\n",
        "    def build(self, hp):\n",
        "        return build_model(hp)\n",
        "\n",
        "    def fit(self, hp, model, X, y, **kwargs):\n",
        "        if hp.Boolean(\"normalize\"):\n",
        "            norm_layer = tf.keras.layers.Normalization()\n",
        "            X = norm_layer(X)\n",
        "        return model.fit(X, y, **kwargs)\n"
      ],
      "metadata": {
        "id": "BNusfFPMgA-w"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#You can then pass an instance of this class to the tuner of your choice, instead of passing the build_model function.\n",
        "#For example, let’s build a kt.Hyperband tuner based on a MyClassificationHyperModel instance:\n",
        "\n",
        "\n",
        "\n",
        "hyperband_tuner = kt.Hyperband(\n",
        "    MyClassificationHyperModel(), objective=\"val_accuracy\", seed=42,\n",
        "    max_epochs=10, factor=3, hyperband_iterations=2,\n",
        "    overwrite=True, directory=\"my_fashion_mnist\", project_name=\"hyperband\")\n"
      ],
      "metadata": {
        "id": "fh35B0F1gLTC"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will execute the Hyperband tuner. We'll also utilize the TensorBoard callback, specifying the root log directory (the tuner manages unique subdirectories for each trial). Additionally, we include an EarlyStopping callback.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ta8-Ux13gtuw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "QKlouTg3gvEh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%load_ext tensorboard\n",
        "#%tensorboard --logdir=./my_logs\n",
        "\n",
        "from pathlib import Path\n",
        "from time import strftime\n",
        "\n",
        "def get_run_logdir(root_logdir=\"my_logs\"):\n",
        "    return Path(root_logdir) / strftime(\"run_%Y_%m_%d_%H_%M_%S\")\n",
        "\n",
        "run_logdir = get_run_logdir()\n",
        "\n",
        "root_logdir = Path(hyperband_tuner.project_dir) / \"tensorboard\"\n",
        "tensorboard_cb = tf.keras.callbacks.TensorBoard(root_logdir)\n",
        "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=2)\n",
        "hyperband_tuner.search(X_train, y_train, epochs=10,\n",
        "                       validation_data=(X_valid, y_valid),\n",
        "                       callbacks=[early_stopping_cb, tensorboard_cb])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-Pjrxxngz_0",
        "outputId": "63bf6d62-f77e-4401-d6bb-27b3a119bea8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 54 Complete [00h 00m 47s]\n",
            "val_accuracy: 0.8479999899864197\n",
            "\n",
            "Best val_accuracy So Far: 0.8705999851226807\n",
            "Total elapsed time: 00h 38m 05s\n",
            "\n",
            "Search: Running Trial #55\n",
            "\n",
            "Value             |Best Value So Far |Hyperparameter\n",
            "7                 |7                 |n_hidden\n",
            "247               |100               |n_neurons\n",
            "0.00039877        |0.00044489        |learning_rate\n",
            "adam              |adam              |optimizer\n",
            "True              |False             |normalize\n",
            "10                |10                |tuner/epochs\n",
            "4                 |4                 |tuner/initial_epoch\n",
            "1                 |2                 |tuner/bracket\n",
            "1                 |2                 |tuner/round\n",
            "0049              |0042              |tuner/trial_id\n",
            "\n",
            "Epoch 5/10\n",
            "   6/1719 [..............................] - ETA: 19s - loss: 0.2948 - accuracy: 0.8906    "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0093s vs `on_train_batch_end` time: 0.0138s). Check your callbacks.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1719/1719 [==============================] - 26s 14ms/step - loss: 0.3424 - accuracy: 0.8754 - val_loss: 0.3906 - val_accuracy: 0.8724\n",
            "Epoch 6/10\n",
            "1719/1719 [==============================] - 23s 14ms/step - loss: 0.3250 - accuracy: 0.8829 - val_loss: 0.3651 - val_accuracy: 0.8752\n",
            "Epoch 7/10\n",
            "1719/1719 [==============================] - 24s 14ms/step - loss: 0.3109 - accuracy: 0.8878 - val_loss: 0.3760 - val_accuracy: 0.8720\n",
            "Epoch 8/10\n",
            "1719/1719 [==============================] - 23s 14ms/step - loss: 0.2938 - accuracy: 0.8946 - val_loss: 0.3449 - val_accuracy: 0.8796\n",
            "Epoch 9/10\n",
            "1204/1719 [====================>.........] - ETA: 7s - loss: 0.2801 - accuracy: 0.8971"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperband is faster than pure random search but still explores hyperparameters randomly, making it a bit coarse. However, Keras Tuner offers a BayesianOptimization tuner, which learns promising hyperparameter regions over time using a probabilistic model called a Gaussian process. It gradually refines the search for the best hyperparameters. Note that this tuner has its own hyperparameters, like alpha for noise level and beta for exploration. The defaults for these hyperparameters are 10^-4 and 2.6, respectively. Besides that, you can use this tuner in a similar way to the previous ones.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NTo1W5Bh3WfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bayesian_opt_tuner = kt.BayesianOptimization(\n",
        "    MyClassificationHyperModel(), objective=\"val_accuracy\", seed=42,\n",
        "    max_trials=10, alpha=1e-4, beta=2.6,\n",
        "    overwrite=True, directory=\"my_fashion_mnist\", project_name=\"bayesian_opt\")\n",
        "bayesian_opt_tuner.search([...])\n",
        "\n",
        "\"\"\"\n",
        "hyperband_tuner.search(X_train, y_train, epochs=10,\n",
        "                       validation_data=(X_valid, y_valid),\n",
        "                       callbacks=[early_stopping_cb, tensorboard_cb])\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 971
        },
        "id": "ZSSa5o_ilwOA",
        "outputId": "010008d2-5372-46db-d9c9-5445c771d465"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 2 Complete [00h 00m 00s]\n",
            "\n",
            "Best val_accuracy So Far: None\n",
            "Total elapsed time: 00h 00m 01s\n",
            "\n",
            "Search: Running Trial #3\n",
            "\n",
            "Value             |Best Value So Far |Hyperparameter\n",
            "4                 |5                 |n_hidden\n",
            "74                |25                |n_neurons\n",
            "0.0090513         |0.00065625        |learning_rate\n",
            "adam              |sgd               |optimizer\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 273, in _try_run_and_update_trial\n",
            "    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 238, in _run_and_update_trial\n",
            "    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/tuner.py\", line 314, in run_trial\n",
            "    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/tuner.py\", line 233, in _build_and_fit_model\n",
            "    results = self.hypermodel.fit(hp, model, *args, **kwargs)\n",
            "TypeError: MyClassificationHyperModel.fit() missing 1 required positional argument: 'y'\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-9438937de6f4>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmax_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2.6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     overwrite=True, directory=\"my_fashion_mnist\", project_name=\"bayesian_opt\")\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mbayesian_opt_tuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/base_tuner.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_run_and_update_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_search_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/base_tuner.py\u001b[0m in \u001b[0;36mon_trial_end\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTrial\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \"\"\"\n\u001b[0;32m--> 338\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/oracle.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mLOCKS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mTHREADS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthread_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mret_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mneed_acquire\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mTHREADS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/oracle.py\u001b[0m in \u001b[0;36mend_trial\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_order\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_consecutive_failures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/oracle.py\u001b[0m in \u001b[0;36m_check_consecutive_failures\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    532\u001b[0m                 \u001b[0mconsecutive_failures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mconsecutive_failures\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_consecutive_failed_trials\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m                 raise RuntimeError(\n\u001b[0m\u001b[1;32m    535\u001b[0m                     \u001b[0;34m\"Number of consecutive failures exceeded the limit \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m                     \u001b[0;34mf\"of {self.max_consecutive_failed_trials}.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Number of consecutive failures exceeded the limit of 3.\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 273, in _try_run_and_update_trial\n    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 238, in _run_and_update_trial\n    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/tuner.py\", line 314, in run_trial\n    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/tuner.py\", line 233, in _build_and_fit_model\n    results = self.hypermodel.fit(hp, model, *args, **kwargs)\nTypeError: MyClassificationHyperModel.fit() missing 1 required positional argument: 'y'\n"
          ]
        }
      ]
    }
  ]
}